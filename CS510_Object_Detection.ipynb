{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS510_Object_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2AcpKUEVUZq"
      },
      "source": [
        "# INITIALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZvrTCDGL1od",
        "outputId": "b8bb51d2-b2fe-44ea-aeeb-ed3ceb8870e4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyksAbtZVhGR"
      },
      "source": [
        "## UNCOMMENT TO CLONE APLHAPOSE SOURCE CODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxgwwh3VVgaj"
      },
      "source": [
        "# !git clone -b pytorch https://github.com/MVIG-SJTU/AlphaPose.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNr4uvHoMEMp",
        "outputId": "b7442d24-f625-4be9-e48f-c9a2549b90ee"
      },
      "source": [
        "%cd /content/gdrive/My Drive/Alphapose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Alphapose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCRrmXJpMbpA",
        "outputId": "94b8a59f-7663-4962-ff2c-3895712c30dc"
      },
      "source": [
        "%cd AlphaPose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Alphapose/AlphaPose\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJVhY6AKWAG5"
      },
      "source": [
        "## PIP INSTALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIYX0kDaV-06"
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1xCNvZiNMqA",
        "outputId": "09e32b3b-9fa2-4b4c-a9a2-fef5c40a57a6"
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting visdom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c9/75/e078f5a2e1df7e0d3044749089fc2823e62d029cc027ed8ae5d71fafcbdc/visdom-0.1.8.9.tar.gz (676kB)\n",
            "\r\u001b[K     |▌                               | 10kB 24.0MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 29.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 25.9MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 23.3MB/s eta 0:00:01\r\u001b[K     |██▍                             | 51kB 25.8MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 19.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 71kB 17.3MB/s eta 0:00:01\r\u001b[K     |███▉                            | 81kB 18.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 92kB 16.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 102kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 112kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 122kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 133kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 143kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 153kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 163kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 174kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 184kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 194kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 204kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 215kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 225kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 235kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 245kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 256kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 266kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 276kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 286kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 296kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 307kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 317kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 327kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 337kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 348kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 358kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 368kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 378kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 389kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 399kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 409kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 419kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 430kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 440kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 450kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 460kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 471kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 481kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 491kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 501kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 512kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 522kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 532kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 542kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 552kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 563kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 573kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 583kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 593kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 604kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 614kB 17.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 624kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 634kB 17.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 645kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 655kB 17.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 665kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 675kB 17.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 686kB 17.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from visdom) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.6/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from visdom) (20.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from visdom) (1.15.0)\n",
            "Collecting jsonpatch\n",
            "  Downloading https://files.pythonhosted.org/packages/af/b3/715016f25700433b2aa017f1a525cccbf052c9385bd7b652d0c083e2fba6/jsonpatch-1.27-py2.py3-none-any.whl\n",
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Collecting websocket-client\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from visdom) (7.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->visdom) (2.10)\n",
            "Collecting jsonpointer>=1.9\n",
            "  Downloading https://files.pythonhosted.org/packages/18/b0/a80d29577c08eea401659254dfaed87f1af45272899e1812d7e01b679bc5/jsonpointer-2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-cp36-none-any.whl size=655251 sha256=80c8e43b1db5a74653e41382d1c5aa80f5efb425fa1a31c50ed07028cf592373\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/19/a7/6d589ed967f4dfefd33bc166d081257bd4ed0cb618dccfd62a\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5711 sha256=bb825041f0c48a4c561ef3f48faaf89c78afc8d8859b59d7a1f7d8469dfab699\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, jsonpatch, torchfile, websocket-client, visdom\n",
            "Successfully installed jsonpatch-1.27 jsonpointer-2.0 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSrEYZlRWKtu"
      },
      "source": [
        "# TO IDENTIFY HUMANS USING ALPHA POSE AND EXTRACT KEYPOINTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcQl9gNjWSqj"
      },
      "source": [
        "## GENERATE ALPHA POSE RESULTS AS JSON ON VIDEO\n",
        "\n",
        "CONFIDENCE AND NMS CAN BE VARIED "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KET4N_mso-ep",
        "outputId": "162f466f-555f-43ae-dbc3-de256de4aefc"
      },
      "source": [
        "!python3 video_demo.py --video $\"/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/1047598234-preview.mp4\" --outdir examples/res/random_videos/1047598234 --conf 0.8 --nms 0.45 \n",
        "# --inp_dim 480 --detbatch 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading YOLO model..\n",
            "Loading pose model from ./models/sppe/duc_se.pth\n",
            "100% 456/456 [00:44<00:00, 10.23it/s]\n",
            "===========================> Finish Model Running.\n",
            "[W CudaIPCTypes.cpp:22] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJE4pxF4WmhC"
      },
      "source": [
        "## GENERATE ALPHA POSE RESULTS AS JSON AS WELL AS SAVE RENDERED VIDEO TO ANALYZE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCME2ZYmOiae",
        "outputId": "f44fce6f-4cbd-4aee-875d-8981f05504c8"
      },
      "source": [
        "!python3 video_demo.py --video $\"/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/trimmed_walk.mp4\" --outdir /content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/1046171221/ --save_video  --matching  --conf 0.7 --nms 0.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading YOLO model..\n",
            "===========================> This video get 211 frames in total.\n",
            "Loading pose model from ./models/sppe/duc_se.pth\n",
            "100% 211/212 [00:17<00:00, 10.95it/s]===========================> Finish Model Running.\n",
            "===========================> Rendering remaining images in the queue...\n",
            "===========================> If this step takes too long, you can enable the --vis_fast flag to use fast rendering (real-time).\n",
            "100% 211/212 [00:37<00:00,  5.69it/s]\n",
            "[W CudaIPCTypes.cpp:22] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksq32ZwIZGBZ"
      },
      "source": [
        "## GENERATE ALPHA POSE RESULTS AS JSON FOR IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLziaw2aZFN4"
      },
      "source": [
        "!python3 demo.py --indir $\"/content/gdrive/My Drive/Alphapose/AlphaPose/examples\" --outdir examples/res "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkzZ0kbiZSyI"
      },
      "source": [
        "## GENERATE ALPHA POSE RESULTS AS JSON FOR IMAGES AND SAVE THE RESULTS FOR ANALYSIS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oRcb8glZZGo"
      },
      "source": [
        "!python3 demo.py --list examples/list-coco-demo.txt --indir $\"/content/gdrive/My Drive/Alphapose/AlphaPose/examples/demo\" --outdir examples/res --save_img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eQshqNpAqpb"
      },
      "source": [
        "# EXTRACT FACE FROM POSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa8vFuJwP3XR"
      },
      "source": [
        "Automated Code with inclusion of Model to detect Mask"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQUywkTqCDPx"
      },
      "source": [
        "# !cd /content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3Rd_GGr3Znx"
      },
      "source": [
        "# !find . -name \"frame*\" \n",
        "# -delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvim_5xWXAFs"
      },
      "source": [
        "## FUNCTION TO CALL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ketn7Il4kxA8"
      },
      "source": [
        "# %%writefile detect_mask_image.py\n",
        "# USAGE\n",
        "# python detect_mask_image.py --image images/pic1.jpeg\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import argparse\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "def mask_image_faster(image_input):\n",
        "\n",
        "\tmodel = load_model(\"/content/gdrive/MyDrive/Alphapose/AlphaPose/MaskDetectionModel/mask_detector_with_blur.model\")\n",
        "  \n",
        "\timage = image_input\n",
        "\t\n",
        "\tprint(\"[INFO] computing face detections...\")\n",
        "\n",
        "\tface = image\n",
        "\tface = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "\tface = cv2.resize(face, (224, 224))\n",
        "\tface = img_to_array(face)\n",
        "\tface = preprocess_input(face)\n",
        "\tface = np.expand_dims(face, axis=0)\n",
        "\n",
        "\t(mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "\tlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\n",
        "\t\n",
        "\tlabel = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100);\n",
        "\n",
        "\treturn label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34WpWmGmXgkz"
      },
      "source": [
        "## EXTRACT FACE WITH HEURISTIC 1 - THIS TRIAL WAS IGNORED AS IT DIDNT PRODUCE BEST RESULTS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0-1VdwrXMuq"
      },
      "source": [
        "###Automated code\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/1048231489/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(len(data))\n",
        "from PIL import Image, ImageDraw\n",
        "total_persons_from_all_frames=len(data)\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/1048231489-preview.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "exception_count=0\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     \n",
        " \n",
        "\n",
        "  prev_person=data[json_count]\n",
        "  person=data[json_count]\n",
        "\n",
        "  print(person['image_id'],prev_person['image_id'],\"frame%d.jpg\"%count)\n",
        "  while person['image_id']==prev_person['image_id']:\n",
        "        leye_x=person['keypoints'][1*3]\n",
        "        leye_y=person['keypoints'][(1*3)+1]\n",
        "        # rshoulder_x=person['keypoints'][6*3]-20\n",
        "        rshoulder_x=person['keypoints'][6*3]\n",
        "        rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "        lshoulder_x=person['keypoints'][5*3]\n",
        "        lshoulder_y=person['keypoints'][(5*3)+1]\n",
        "        x1=math.floor(leye_x+20)\n",
        "        y1=math.floor(leye_y-(rshoulder_y-leye_y+20))\n",
        "        x2=math.floor(rshoulder_x-20)\n",
        "        y2=math.floor(rshoulder_y)\n",
        "     \n",
        "        # start_point = (x1,y1) \n",
        "        # end_point = (x2,y2)\n",
        "        if y1<y2:\n",
        "            start_point = (x1,y1) \n",
        "      \n",
        "            end_point = (x2,y2)\n",
        "        elif y1>y2:\n",
        "            start_point = (x1,y2) \n",
        "     \n",
        "            end_point = (x2,y1)\n",
        "\n",
        "        try:\n",
        "          crop_img = image[ y1:y2,x2:x1]\n",
        "          \n",
        "          label=(mask_image_faster(crop_img))\n",
        "          if 'No Mask' in label:\n",
        "              color=(0, 0,255) \n",
        "          else:\n",
        "              color=(0, 255,0) \n",
        "          image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "          org = (x1, y1) \n",
        "    \n",
        "          image = cv2.putText(image, label, org, font,  \n",
        "                    fontScale, color, thickness, cv2.LINE_AA)\n",
        "        except Exception as err:\n",
        "          print(err, str(start_point), str(end_point),\"x1-->y1\",str(x1),str(y1),\"x2-->y2\",str(x2),str(y2))\n",
        "          cv2_imshow( image)\n",
        "          exception_count+=1\n",
        "       \n",
        "        print('json count'+ str(json_count))\n",
        "        json_count+=1\n",
        "\n",
        "        \n",
        "        prev_person=person\n",
        "        try:\n",
        "          person=data[json_count]\n",
        "        except:\n",
        "          break\n",
        "  \n",
        " \n",
        "  iter+=1\n",
        "  count += 1\n",
        "  print('exception count is' + str(exception_count))\n",
        "\n",
        "  cv2.imwrite(\"/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/Output_Frames/frame%d.jpg\" % count, image)\n",
        "  success,image = vidcap.read()\n",
        "  \n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx4gMMofXvBK"
      },
      "source": [
        "## EXTRACT FACE WITH HEURISTIC 2 - THIS TRIAL WAS IGNORED AS IT DIDNT PRODUCE BEST RESULTS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dHxnCEsMSWb"
      },
      "source": [
        "###Automated code\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/1047598234/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(len(data))\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/1047598234-preview.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     \n",
        " \n",
        "\n",
        "  prev_person=data[json_count]\n",
        "  person=data[json_count]\n",
        "\n",
        "  print(person['image_id'],prev_person['image_id'],\"frame%d.jpg\"%count)\n",
        "  while person['image_id']==prev_person['image_id']:\n",
        "        leye_x=person['keypoints'][1*3]\n",
        "        leye_y=person['keypoints'][(1*3)+1]\n",
        "        # rshoulder_x=person['keypoints'][6*3]-20\n",
        "        rshoulder_x=person['keypoints'][6*3]\n",
        "        rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "        lshoulder_x=person['keypoints'][5*3]\n",
        "        lshoulder_y=person['keypoints'][(5*3)+1]\n",
        "        x1=math.floor(lshoulder_x)\n",
        "        y1=math.floor(lshoulder_y)\n",
        "        x2=math.floor(rshoulder_x)\n",
        "        y2=math.floor(rshoulder_y)\n",
        "        start_point = (x1,math.floor(y1+(x2-x1)+(0.2*(x2-x1))))\n",
        "        end_point = (x2,y2)\n",
        "\n",
        "\n",
        "        # start_point = (x1,y1) \n",
        "        # end_point = (x2,y2)\n",
        "        # if y1<y2:\n",
        "        #     start_point = (x1,y1) \n",
        "      \n",
        "        #     end_point = (x2,y2)\n",
        "        # elif y1>y2:\n",
        "        #     start_point = (x1,y2) \n",
        "     \n",
        "        #     end_point = (x2,y1)\n",
        "\n",
        "        try:\n",
        "          crop_img = image[ y1:y2,x2:x1]\n",
        "          \n",
        "          label=(mask_image_faster(crop_img))\n",
        "          if 'No Mask' in label:\n",
        "              color=(0, 0,255) \n",
        "          else:\n",
        "              color=(0, 255,0) \n",
        "          image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "          org = (x1, y1) \n",
        "    \n",
        "          image = cv2.putText(image, label, org, font,  \n",
        "                    fontScale, color, thickness, cv2.LINE_AA)\n",
        "        except Exception as err:\n",
        "          print(err)\n",
        "        # if iter>6:\n",
        "        try:\n",
        "            # print(start_point,end_point,color,thickness)\n",
        "            # cv2_imshow( crop_img)\n",
        "            cv2.imwrite('/content/sample_data/cropped_images/crop_img_'+str(json_count)+'.jpg', crop_img) \n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "            # continue\n",
        "        print('json count'+ str(json_count))\n",
        "        json_count+=1\n",
        "\n",
        "        \n",
        "        prev_person=person\n",
        "        try:\n",
        "          person=data[json_count]\n",
        "        except:\n",
        "          break\n",
        "  \n",
        " \n",
        "  iter+=1\n",
        "  count += 1\n",
        "  cv2_imshow(image)\n",
        "  cv2.imwrite(\"/content/sample_data/Output_Frames/frame%d.jpg\" % count, image)\n",
        "  success,image = vidcap.read()\n",
        "  \n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slu5hUA3X3IS"
      },
      "source": [
        "## EXTRACT FACE WITH HEURISTIC 3 - THIS TRIAL WAS USED AS IT PRODUCES BEST RESULTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZBCW7F9P7jD"
      },
      "source": [
        "###Automated code\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/1048231489/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(len(data))\n",
        "from PIL import Image, ImageDraw\n",
        "total_persons_from_all_frames=len(data)\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/1048231489-preview.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "exception_count=0\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     \n",
        " \n",
        "\n",
        "  prev_person=data[json_count]\n",
        "  person=data[json_count]\n",
        "\n",
        "  print(person['image_id'],prev_person['image_id'],\"frame%d.jpg\"%count)\n",
        "  while person['image_id']==prev_person['image_id']:\n",
        "        leye_x=person['keypoints'][1*3]\n",
        "        leye_y=person['keypoints'][(1*3)+1]\n",
        "        # rshoulder_x=person['keypoints'][6*3]-20\n",
        "        rshoulder_x=person['keypoints'][6*3]\n",
        "        rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "        lshoulder_x=person['keypoints'][5*3]\n",
        "        lshoulder_y=person['keypoints'][(5*3)+1]\n",
        "        # x1=math.floor(leye_x+20)\n",
        "        # x1=math.floor(lshoulder_x+20)\n",
        "        x1=math.floor(lshoulder_x)\n",
        "        # y1=math.floor(leye_y-(rshoulder_y-leye_y+20))\n",
        "        y1=math.floor(leye_y-(rshoulder_y-leye_y))\n",
        "        # x2=math.floor(rshoulder_x-20)\n",
        "        x2=math.floor(rshoulder_x)\n",
        "        y2=math.floor(rshoulder_y)\n",
        "     \n",
        "        # start_point = (x1,y1) \n",
        "        # end_point = (x2,y2)\n",
        "        if y1<y2:\n",
        "            start_point = (x1,y1) \n",
        "      \n",
        "            end_point = (x2,y2)\n",
        "        elif y1>y2:\n",
        "            start_point = (x1,y2) \n",
        "     \n",
        "            end_point = (x2,y1)\n",
        "\n",
        "        try:\n",
        "          crop_img = image[ y1:y2,x2:x1]\n",
        "          \n",
        "          label=(mask_image_faster(crop_img))\n",
        "          if 'No Mask' in label:\n",
        "              color=(0, 0,255) \n",
        "          else:\n",
        "              color=(0, 255,0) \n",
        "          image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "          org = (x1, y1) \n",
        "    \n",
        "          image = cv2.putText(image, label, org, font,  \n",
        "                    fontScale, color, thickness, cv2.LINE_AA)\n",
        "        except Exception as err:\n",
        "          print(err, str(start_point), str(end_point),\"x1-->y1\",str(x1),str(y1),\"x2-->y2\",str(x2),str(y2))\n",
        "          cv2_imshow( image)\n",
        "          exception_count+=1\n",
        "        # if iter>6:\n",
        "        # try:\n",
        "        #     # print(start_point,end_point,color,thickness)\n",
        "        #     # cv2_imshow( crop_img)\n",
        "        #     cv2.imwrite('/content/sample_data/cropped_images/crop_img_'+str(json_count)+'.jpg', crop_img) \n",
        "        # except Exception as ex:\n",
        "        #     print(ex)\n",
        "        #     exception_count+=1\n",
        "        #     # continue\n",
        "        print('json count'+ str(json_count))\n",
        "        json_count+=1\n",
        "\n",
        "        \n",
        "        prev_person=person\n",
        "        try:\n",
        "          person=data[json_count]\n",
        "        except:\n",
        "          break\n",
        "  \n",
        " \n",
        "  iter+=1\n",
        "  count += 1\n",
        "  print('exception count is' + str(exception_count))\n",
        "#   cv2_imshow(image)\n",
        "  cv2.imwrite(\"/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/Output_Frames/frame%d.jpg\" % count, image)\n",
        "  success,image = vidcap.read()\n",
        "  \n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGWDJazuxOl-"
      },
      "source": [
        "#  !rm -r /content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/Output_Frames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVDHI1yMYDlT"
      },
      "source": [
        "# REGENERATE VIDEO FROM FRAMES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyv5UoHzhn36"
      },
      "source": [
        "import re\n",
        "\n",
        "def tryint(s):\n",
        "    try:\n",
        "        return int(s)\n",
        "    except ValueError:\n",
        "        return s\n",
        "    \n",
        "def alphanum_key(s):\n",
        "    \"\"\" Turn a string into a list of string and number chunks.\n",
        "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
        "    \"\"\"\n",
        "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
        "\n",
        "def sort_nicely(l):\n",
        "    \"\"\" Sort the given list in the way that humans expect.\n",
        "    \"\"\"\n",
        "    l.sort(key=alphanum_key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jlkW36rgH6g",
        "outputId": "20fbeeb4-fdc0-4355-8db6-94ae140e896b"
      },
      "source": [
        "#generate Video\n",
        "import os\n",
        "import cv2\n",
        "def generate_video(): \n",
        "    image_folder = '/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/Output_Frames' # make sure to use your folder \n",
        "    video_name = 'full_scale_video_integration.avi'\n",
        "    os.chdir(\"/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/\") \n",
        "      \n",
        "    images = [img for img in os.listdir(image_folder) \n",
        "              if img.endswith(\".jpg\") or\n",
        "                 img.endswith(\".jpeg\") or\n",
        "                 img.endswith(\"png\")] \n",
        "    sort_nicely(images)\n",
        "    # Array images should only consider \n",
        "    # the image files ignoring others if any \n",
        "    print(images)  \n",
        "  \n",
        "    frame = cv2.imread(os.path.join(image_folder, images[0])) \n",
        "  \n",
        "    # setting the frame width, height width \n",
        "    # the width, height of first image \n",
        "    height, width, layers = frame.shape   \n",
        "  \n",
        "    video = cv2.VideoWriter(video_name, 0, 1, (width, height))  \n",
        "  \n",
        "    # Appending the images to the video one by one \n",
        "    for image in images:  \n",
        "        video.write(cv2.imread(os.path.join(image_folder, image)))  \n",
        "      \n",
        "    # Deallocating memories taken for window creation \n",
        "    cv2.destroyAllWindows()  \n",
        "    video.release()  # releasing the video generated \n",
        "  \n",
        "  \n",
        "# Calling the generate_video function \n",
        "generate_video() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['frame1.jpg', 'frame2.jpg', 'frame3.jpg', 'frame4.jpg', 'frame5.jpg', 'frame6.jpg', 'frame7.jpg', 'frame8.jpg', 'frame9.jpg', 'frame10.jpg', 'frame11.jpg', 'frame12.jpg', 'frame13.jpg', 'frame14.jpg', 'frame15.jpg', 'frame16.jpg', 'frame17.jpg', 'frame18.jpg', 'frame19.jpg', 'frame20.jpg', 'frame21.jpg', 'frame22.jpg', 'frame23.jpg', 'frame24.jpg', 'frame25.jpg', 'frame26.jpg', 'frame27.jpg', 'frame28.jpg', 'frame29.jpg', 'frame30.jpg', 'frame31.jpg', 'frame32.jpg', 'frame33.jpg', 'frame34.jpg', 'frame35.jpg', 'frame36.jpg', 'frame37.jpg', 'frame38.jpg', 'frame39.jpg', 'frame40.jpg', 'frame41.jpg', 'frame42.jpg', 'frame43.jpg', 'frame44.jpg', 'frame45.jpg', 'frame46.jpg', 'frame47.jpg', 'frame48.jpg', 'frame49.jpg', 'frame50.jpg', 'frame51.jpg', 'frame52.jpg', 'frame53.jpg', 'frame54.jpg', 'frame55.jpg', 'frame56.jpg', 'frame57.jpg', 'frame58.jpg', 'frame59.jpg', 'frame60.jpg', 'frame61.jpg', 'frame62.jpg', 'frame63.jpg', 'frame64.jpg', 'frame65.jpg', 'frame66.jpg', 'frame67.jpg', 'frame68.jpg', 'frame69.jpg', 'frame70.jpg', 'frame71.jpg', 'frame72.jpg', 'frame73.jpg', 'frame74.jpg', 'frame75.jpg', 'frame76.jpg', 'frame77.jpg', 'frame78.jpg', 'frame79.jpg', 'frame80.jpg', 'frame81.jpg', 'frame82.jpg', 'frame83.jpg', 'frame84.jpg', 'frame85.jpg', 'frame86.jpg', 'frame87.jpg', 'frame88.jpg', 'frame89.jpg', 'frame90.jpg', 'frame91.jpg', 'frame92.jpg', 'frame93.jpg', 'frame94.jpg', 'frame95.jpg', 'frame96.jpg', 'frame97.jpg', 'frame98.jpg', 'frame99.jpg', 'frame100.jpg', 'frame101.jpg', 'frame102.jpg', 'frame103.jpg', 'frame104.jpg', 'frame105.jpg', 'frame106.jpg', 'frame107.jpg', 'frame108.jpg', 'frame109.jpg', 'frame110.jpg', 'frame111.jpg', 'frame112.jpg', 'frame113.jpg', 'frame114.jpg', 'frame115.jpg', 'frame116.jpg', 'frame117.jpg', 'frame118.jpg', 'frame119.jpg', 'frame120.jpg', 'frame121.jpg', 'frame122.jpg', 'frame123.jpg', 'frame124.jpg', 'frame125.jpg', 'frame126.jpg', 'frame127.jpg', 'frame128.jpg', 'frame129.jpg', 'frame130.jpg', 'frame131.jpg', 'frame132.jpg', 'frame133.jpg', 'frame134.jpg', 'frame135.jpg', 'frame136.jpg', 'frame137.jpg', 'frame138.jpg', 'frame139.jpg', 'frame140.jpg', 'frame141.jpg', 'frame142.jpg', 'frame143.jpg', 'frame144.jpg', 'frame145.jpg', 'frame146.jpg', 'frame147.jpg', 'frame148.jpg', 'frame149.jpg', 'frame150.jpg', 'frame151.jpg', 'frame152.jpg', 'frame153.jpg', 'frame154.jpg', 'frame155.jpg', 'frame156.jpg', 'frame157.jpg', 'frame158.jpg', 'frame159.jpg', 'frame160.jpg', 'frame161.jpg', 'frame162.jpg', 'frame163.jpg', 'frame164.jpg', 'frame165.jpg', 'frame166.jpg', 'frame167.jpg', 'frame168.jpg', 'frame169.jpg', 'frame170.jpg', 'frame171.jpg', 'frame172.jpg', 'frame173.jpg', 'frame174.jpg', 'frame175.jpg', 'frame176.jpg', 'frame177.jpg', 'frame178.jpg', 'frame179.jpg', 'frame180.jpg']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMxHCv1YL2ZU"
      },
      "source": [
        "# TROUBLESHOOTING SECTION\n",
        "\n",
        "DIFFERENT DEBUG PARAMETERS USED DURING DEVELOPMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlQ_u1bbL1-_"
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/1048231489/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "\n",
        "\n",
        "#read each line\n",
        "person=data[112]\n",
        "print(person)\n",
        "#read number of keypoints / divide by 3 - gives number of keypoints\n",
        "\n",
        "print(len(person['keypoints'])/3)\n",
        "\n",
        "#read leye-1, reye-2, lshoulder-5, rshoulder-6 coordinates\n",
        "\n",
        "print('nose-->'+str(person['keypoints'][0])+\",\"+\n",
        "      \n",
        "      str(person['keypoints'][(0)+1]))\n",
        "\n",
        "print('leye-->'+str(person['keypoints'][1*3])+\",\"+\n",
        "      \n",
        "      str(person['keypoints'][(1*3)+1]),str(person['keypoints'][(1*3)+2]))\n",
        "\n",
        "print('reye-->'+str(person['keypoints'][2*3])+\",\"+\n",
        "      \n",
        "      str(person['keypoints'][(2*3)+1]),str(person['keypoints'][(2*3)+2]))\n",
        "\n",
        "print('lshoulder-->'+str(person['keypoints'][5*3])+\",\"+\n",
        "      \n",
        "      str(person['keypoints'][(5*3)+1]),str(person['keypoints'][(5*3)+2]))\n",
        "\n",
        "print('rshoulder-->'+str(person['keypoints'][6*3])+\",\"+\n",
        "      \n",
        "      str(person['keypoints'][(6*3)+1]),str(person['keypoints'][(6*3)+2]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEz1QtcYM038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b53b0a3-243e-431e-a3d3-7adfe2fc75b9"
      },
      "source": [
        "import cv2\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/1048231489-preview.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     \n",
        " \n",
        "\n",
        "  prev_person=data[json_count]\n",
        "  person=data[json_count]\n",
        "\n",
        "  print(person['image_id'],prev_person['image_id'],\"frame%d.jpg\"%count)\n",
        "  while person['image_id']==prev_person['image_id'] and json_count<113:\n",
        "        if json_count==112:\n",
        "            leye_x=person['keypoints'][1*3]\n",
        "            leye_y=person['keypoints'][(1*3)+1]\n",
        "            rshoulder_x=person['keypoints'][6*3]-20\n",
        "            rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "            x1=math.floor(leye_x+20)\n",
        "            y1=math.floor(leye_y-(rshoulder_y-leye_y+20))\n",
        "            x2=math.floor(rshoulder_x-20)\n",
        "            y2=math.floor(rshoulder_y)\n",
        "        \n",
        "            # start_point = (x1,y1) \n",
        "            # end_point = (x2,y2)\n",
        "            if y1<y2:\n",
        "                start_point = (x1,y1) \n",
        "        \n",
        "                end_point = (x2,y2)\n",
        "            elif y1>y2:\n",
        "                start_point = (x1,y2) \n",
        "        \n",
        "                end_point = (x2,y1)\n",
        "            image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "            cv2_imshow( image)\n",
        "            try:\n",
        "                crop_img = image[ y1:y2,x2:x1]\n",
        "                cv2_imshow( crop_img)\n",
        "                # face=cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)\n",
        "               \n",
        "                print(mask_image_faster(crop_img))\n",
        "\n",
        "            except Exception as err:\n",
        "                print(err)\n",
        "                json_count+=1\n",
        "       \n",
        "            \n",
        "        print('json count'+ str(json_count))\n",
        "        json_count+=1\n",
        "\n",
        "\n",
        "\n",
        "        prev_person=person\n",
        "        try:\n",
        "          person=data[json_count]\n",
        "        except:\n",
        "          break\n",
        "  \n",
        " \n",
        "  iter+=1\n",
        "  count += 1\n",
        "  success,image = vidcap.read()\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXpipMMIAkR9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "07a6ed43-c350-4cf5-cdee-dd071a8806d4"
      },
      "source": [
        "TROUBLSHOOTING- SATRUDAY"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ed9c7c09dee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTROUBLSHOOTING\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mSATRUDAY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'TROUBLSHOOTING' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a01de2ZcAnKR"
      },
      "source": [
        "###Automated code\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/1048231489/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(len(data))\n",
        "from PIL import Image, ImageDraw\n",
        "total_persons_from_all_frames=len(data)\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/1048231489-preview.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "exception_count=0\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     \n",
        " \n",
        "\n",
        "  prev_person=data[json_count]\n",
        "  person=data[json_count]\n",
        "  image_final=image.copy()\n",
        "  print(person['image_id'],prev_person['image_id'],\"frame%d.jpg\"%count)\n",
        "  while person['image_id']==prev_person['image_id']:\n",
        "        leye_x=person['keypoints'][1*3]\n",
        "        leye_y=person['keypoints'][(1*3)+1]\n",
        "        # rshoulder_x=person['keypoints'][6*3]-20\n",
        "        rshoulder_x=person['keypoints'][6*3]\n",
        "        rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "        lshoulder_x=person['keypoints'][5*3]\n",
        "        lshoulder_y=person['keypoints'][(5*3)+1]\n",
        "        # x1=math.floor(leye_x+20)\n",
        "        # x1=math.floor(lshoulder_x+20)\n",
        "        x1=math.floor(lshoulder_x)\n",
        "        # y1=math.floor(leye_y-(rshoulder_y-leye_y+20))\n",
        "        y1=math.floor(leye_y-(rshoulder_y-leye_y))\n",
        "        # x2=math.floor(rshoulder_x-20)\n",
        "        x2=math.floor(rshoulder_x)\n",
        "        y2=math.floor(rshoulder_y)\n",
        "     \n",
        "        # start_point = (x1,y1) \n",
        "        # end_point = (x2,y2)\n",
        "        if y1<y2:\n",
        "            start_point = (x1,y1) \n",
        "      \n",
        "            end_point = (x2,y2)\n",
        "        elif y1>y2:\n",
        "            start_point = (x1,y2) \n",
        "     \n",
        "            end_point = (x2,y1)\n",
        "\n",
        "        try:\n",
        "            \n",
        "          if y1<y2:\n",
        "            print(str(y1),\":\",str(y2), str(x2),\":\",str(x1))\n",
        "            crop_img = image[y1:y2, x2:x1]\n",
        "        \n",
        "        #   elif y1<y2 and x1>x2:\n",
        "        #       print(str(x2),\":\",str(x1), str(y1),\":\",str(y2))\n",
        "        #       crop_img = image[ x2:x1,y1:y2]\n",
        "          \n",
        "        #   elif y1>y2 and x1<x2:\n",
        "        #      print(str(x1),\":\",str(x2), str(y2),\":\",str(y1))\n",
        "        #      crop_img=image[x1:x2,y2:y1 ]\n",
        "             \n",
        "          \n",
        "          else:\n",
        "              print(str(y2),\":\",str(y1), str(x2),\":\",str(x1))\n",
        "              crop_img=image[y2:y1,x2:x1]\n",
        "          \n",
        "        #   print(\"display crop\")\n",
        "        #   cv2_imshow(crop_img)\n",
        "          label=(mask_image_faster(crop_img))\n",
        "          if 'No Mask' in label:\n",
        "              color=(0, 0,255) \n",
        "          else:\n",
        "              color=(0, 255,0) \n",
        "          image_final = cv2.rectangle(image_final, start_point, end_point, color, thickness)\n",
        "          org = (x1, y1) \n",
        "    \n",
        "          image_final = cv2.putText(image_final, label, org, font,  \n",
        "                    fontScale, color, thickness, cv2.LINE_AA)\n",
        "        except Exception as err:\n",
        "          print(err, str(start_point), str(end_point),\"x1-->y1\",str(x1),str(y1),\"x2-->y2\",str(x2),str(y2),\"rshoulder_x-->\",str(rshoulder_x))\n",
        "          cv2_imshow( image_final)\n",
        "          exception_count+=1\n",
        "        # if iter>6:\n",
        "        # try:\n",
        "        #     # print(start_point,end_point,color,thickness)\n",
        "        #     # cv2_imshow( crop_img)\n",
        "        #     cv2.imwrite('/content/sample_data/cropped_images/crop_img_'+str(json_count)+'.jpg', crop_img) \n",
        "        # except Exception as ex:\n",
        "        #     print(ex)\n",
        "        #     exception_count+=1\n",
        "        #     # continue\n",
        "        print('json count'+ str(json_count))\n",
        "        json_count+=1\n",
        "\n",
        "        \n",
        "        prev_person=person\n",
        "        try:\n",
        "          person=data[json_count]\n",
        "        except:\n",
        "          break\n",
        "  \n",
        " \n",
        "  iter+=1\n",
        "  count += 1\n",
        "  print('exception count is' + str(exception_count))\n",
        "#   cv2_imshow(image)\n",
        "  cv2.imwrite(\"/content/sample_data/Output_Frames/frame%d.jpg\" % count, image_final)\n",
        "  success,image = vidcap.read()\n",
        "  \n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slxFCmL7KfMq"
      },
      "source": [
        "###Automated code\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/trafficsignal/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(len(data))\n",
        "from PIL import Image, ImageDraw\n",
        "total_persons_from_all_frames=len(data)\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/traffic_trimmed.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "exception_count=0\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     \n",
        " \n",
        "\n",
        "  prev_person=data[json_count]\n",
        "  person=data[json_count]\n",
        "  image_final=image.copy()\n",
        "  \n",
        "  print(person['image_id'],prev_person['image_id'],\"frame%d.jpg\"%count)\n",
        "  while person['image_id']==prev_person['image_id']:\n",
        "        leye_x=person['keypoints'][1*3]\n",
        "        leye_y=person['keypoints'][(1*3)+1]\n",
        "        # rshoulder_x=person['keypoints'][6*3]-20\n",
        "        rshoulder_x=person['keypoints'][6*3]\n",
        "        rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "        lshoulder_x=person['keypoints'][5*3]\n",
        "        lshoulder_y=person['keypoints'][(5*3)+1]\n",
        "        # x1=math.floor(leye_x+20)\n",
        "        # x1=math.floor(lshoulder_x+20)\n",
        "        x1=math.floor(lshoulder_x)\n",
        "        # y1=math.floor(leye_y-(rshoulder_y-leye_y+20))\n",
        "        y1=math.floor(leye_y-(rshoulder_y-leye_y))\n",
        "        # x2=math.floor(rshoulder_x-20)\n",
        "        x2=math.floor(rshoulder_x)\n",
        "        y2=math.floor(rshoulder_y)\n",
        "        show_image=False\n",
        "        # start_point = (x1,y1) \n",
        "        # end_point = (x2,y2)\n",
        "        if x1==x2:\n",
        "             show_image=True\n",
        "             x1 +=50\n",
        "             x2 -=50\n",
        "        if y1<y2:\n",
        "            start_point = (x1,y1) \n",
        "      \n",
        "            end_point = (x2,y2)\n",
        "        elif y1>y2:\n",
        "            start_point = (x1,y2) \n",
        "     \n",
        "            end_point = (x2,y1)\n",
        "\n",
        "        try:\n",
        "        \n",
        "          if y1<y2:\n",
        "            print(str(y1),\":\",str(y2), str(x2),\":\",str(x1))\n",
        "            crop_img = image[y1:y2, x2:x1]\n",
        "            # if(show_image):\n",
        "            #     cv2_imshow(crop_img)\n",
        "\n",
        "          else:\n",
        "              print(str(y2),\":\",str(y1), str(x2),\":\",str(x1))\n",
        "              crop_img=image[y2:y1,x2:x1]\n",
        "          \n",
        "        #   print(\"display crop\")\n",
        "        #   cv2_imshow(crop_img)\n",
        "          \n",
        "          label=(mask_image_faster(crop_img))\n",
        "          if 'No Mask' in label:\n",
        "              color=(0, 0,255) \n",
        "          else:\n",
        "              color=(0, 255,0) \n",
        "        #   image_final = cv2.rectangle(image_final, start_point, end_point, color, thickness)\n",
        "        #   if(show_image):\n",
        "          image_final = cv2.rectangle(image_final, start_point, end_point, color, thickness)\n",
        "          org = (x1, y1) \n",
        "        \n",
        "          image_final = cv2.putText(image_final, label, org, font,  \n",
        "                        fontScale, color, thickness, cv2.LINE_AA)\n",
        "            # cv2_imshow(image_final)\n",
        "        except Exception as err:\n",
        "          print(err, str(start_point), str(end_point),\"x1-->y1\",str(x1),str(y1),\"x2-->y2\",str(x2),str(y2),\"rshoulder_x-->\",str(rshoulder_x))\n",
        "        #   cv2_imshow( image_final)\n",
        "          exception_count+=1\n",
        "\n",
        "        print('json count'+ str(json_count))\n",
        "        json_count+=1\n",
        "\n",
        "        \n",
        "        prev_person=person\n",
        "        try:\n",
        "          person=data[json_count]\n",
        "        except:\n",
        "          break\n",
        "  \n",
        " \n",
        "  iter+=1\n",
        "  count += 1\n",
        "  print('exception count is' + str(exception_count))\n",
        "\n",
        "  cv2.imwrite(\"/content/sample_data/Output_Frames/frame%d.jpg\" % count, image_final)\n",
        "  success,image = vidcap.read()\n",
        "  \n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCNwAkCmZly4"
      },
      "source": [
        "import cv2\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/79.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "# while success:\n",
        "while iter<2:\n",
        "  cv2.imwrite(\"frame%d.jpg\" % count, image)     # save frame as JPEG file      \n",
        "  success,image = vidcap.read()\n",
        "  window_name = 'Image'\n",
        "    \n",
        "  # font \n",
        "  font = cv2.FONT_HERSHEY_SIMPLEX \n",
        "  x41=1115+20\n",
        "  y41=534-(550-534+20)\n",
        "  x42=1087-20\n",
        "  y42=550\n",
        "  start_point_4 = (x41,y41) \n",
        "  end_point_4 = (x42,y42)\n",
        "\n",
        "# print('leye-->'+str(person['keypoints'][1*3])+\",\"+str(person['keypoints'][(1*3)+1]))\n",
        "\n",
        "# print('reye-->'+str(person['keypoints'][2*3])+\",\"+str(person['keypoints'][(2*3)+1]))\n",
        "\n",
        "# print('lshoulder-->'+str(person['keypoints'][5*3])+\",\"+str(person['keypoints'][(5*3)+1]))\n",
        "\n",
        "# print('rshoulder-->'+str(person['keypoints'][6*3])+\",\"+str(person['keypoints'][(6*3)+1]))\n",
        "\n",
        "# nose-->447.424072265625,527.6403198242188\n",
        "# leye-->451.19146728515625,523.8729248046875\n",
        "# reye-->441.7729797363281,523.8729248046875\n",
        "# lshoulder-->464.3773498535156,554.0120849609375\n",
        "# rshoulder-->419.16864013671875,554.0120849609375\n",
        "\n",
        "# leye_x=person['keypoints'][1*3]\n",
        "#         leye_y=person['keypoints'][(1*3)+1]\n",
        "#         rshoulder_x=person['keypoints'][6*3]-20\n",
        "#         rshoulder_y=person['keypoints'][(6*3)+1]\n",
        "#         x1=math.floor(leye_x+20)\n",
        "#         y1=math.floor(leye_y-(rshoulder_y-leye_y+20))\n",
        "#         x2=math.floor(rshoulder_x-20)\n",
        "#         y2=math.floor(rshoulder_y)\n",
        "\n",
        "# nose-->580.2733764648438,502.3037109375\n",
        "# leye-->587.6251831054688,494.9519348144531\n",
        "# reye-->572.921630859375,494.9519348144531\n",
        "# lshoulder-->583.9492797851562,542.7384033203125\n",
        "# rshoulder-->503.07989501953125,542.7384033203125\n",
        "\n",
        "\n",
        "# leye-->1630.1387939453125,688.5223999023438\n",
        "# reye-->1636.74560546875,689.8438110351562\n",
        "# lshoulder-->1619.56787109375,590.7412719726562\n",
        "# rshoulder-->1649.959228515625,588.0985107421875\n",
        "\n",
        "# x1=1650 -----------x1=607\n",
        "# x2=1629  -----------x2=483\n",
        "# y2=588----------------y2=542\n",
        "# y1=688-(588-688+20)=768-----y1= 426\n",
        "start_point = (x1,y1) (1650,1710) \n",
        "  end_point = (x2,y2) (1629,588)\n",
        "  image[ y1:y2,x2:x1]\n",
        "   image[ 768:588,1629:1650] [ 426:542, 483,607]\n",
        "\n",
        "  x31=451+20\n",
        "  y31=523-(554-523+20)\n",
        "  x32=419-20\n",
        "  y32=554\n",
        "  start_point_3 = (x31,y31) \n",
        "  end_point_3 = (x32,y32)\n",
        "\n",
        "  x01=587+20\n",
        "  y01=494-(542-494+20)\n",
        "  x02=503-20\n",
        "  y02=542\n",
        "  start_point_0 = (x01,y01) \n",
        "  end_point_0 = (x02,y02)\n",
        "\n",
        "  x1=876+20\n",
        "  y1=546-(581-546+20)\n",
        "  x2=791-20\n",
        "  y2=575\n",
        "  start_point = (x1,y1) \n",
        "  end_point = (x2,y2)\n",
        "\n",
        "  x11=1014+20\n",
        "  y11=517-(547-517+20)\n",
        "  x12=984-20\n",
        "  y12=551\n",
        "  start_point_2 = (x11,y11) \n",
        "  end_point_2 = (x12,y12)\n",
        "  # fontScale \n",
        "  fontScale = 1\n",
        "    \n",
        "  # Blue color in BGR \n",
        "\n",
        "    \n",
        "  # Line thickness of 2 px \n",
        "  thickness = 2\n",
        "    \n",
        "  # Using cv2.putText() method \n",
        "  image = cv2.rectangle(image, start_point_0, end_point_0, color, thickness)\n",
        "  image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "  image = cv2.rectangle(image, start_point_2, end_point_2, color, thickness)\n",
        "  image = cv2.rectangle(image, start_point_3, end_point_3, color, thickness)\n",
        "  image = cv2.rectangle(image, start_point_4, end_point_4, color, thickness)\n",
        "  # print('Read a new frame: ', success)\n",
        "  cv2_imshow( image)\n",
        "  cv2.imwrite('full.jpg', image) \n",
        "  # crop_img = image[ 546-40:575+20,791-20:876+20]\n",
        "  crop_img = image[ y1:y2,x2:x1]\n",
        "  crop_img_2 = image[ y11:y12,x12:x11]\n",
        "  crop_img_0 = image[ y01:y02,x02:x01]\n",
        "  crop_img_3 = image[ y31:y32,x32:x31]\n",
        "  crop_img_4 = image[ y41:y42,x42:x41]\n",
        "  cv2_imshow( crop_img)\n",
        "  cv2.imwrite('crop_img_1.jpg', crop_img) \n",
        "  cv2_imshow( crop_img_2)\n",
        "  cv2.imwrite('crop_img_2.jpg', crop_img_2) \n",
        "  cv2_imshow( crop_img_0)\n",
        "  cv2.imwrite('crop_img_0.jpg', crop_img_0) \n",
        "  cv2_imshow( crop_img_3)\n",
        "  cv2.imwrite('crop_img_3.jpg', crop_img_3) \n",
        "  cv2_imshow( crop_img_4)\n",
        "  cv2.imwrite('crop_img_4.jpg', crop_img_4) \n",
        "  \n",
        "  iter+=1\n",
        "  count += 1\n",
        "print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xheyJe3zZ3VQ"
      },
      "source": [
        "###Automated code\n",
        "import cv2\n",
        "import json\n",
        "import math\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/res/random_videos/trafficsignal/alphapose-results.json') as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "print(len(data))\n",
        "from PIL import Image, ImageDraw\n",
        "total_persons_from_all_frames=len(data)\n",
        "from google.colab.patches import cv2_imshow\n",
        "vidcap = cv2.VideoCapture('/content/gdrive/MyDrive/Alphapose/AlphaPose/examples/Video/traffic_trimmed.mp4')\n",
        "success,image = vidcap.read()\n",
        "count = 0\n",
        "iter=0\n",
        "json_count=0\n",
        "exception_count=0\n",
        "window_name = 'Image'\n",
        "font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "fontScale = 1\n",
        "thickness = 2\n",
        "color = (255, 0, 0) \n",
        "# while success:\n",
        "while success :\n",
        "  cv2.imwrite(\"/content/sample_data/frame%d.jpg\" % count, image)     \n",
        "  success,image = vidcap.read()\n",
        "  count+=1\n",
        "  \n",
        "print(count)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yH2eS4sCezjr"
      },
      "source": [
        "# WEB CAM EXPERIMENT - NOT INTEGRATED INTO CODE FULLY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCkyG3ZGfkGf"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwPtbYGmfkGf"
      },
      "source": [
        "from IPython.display import Image\n",
        "while 1:\n",
        "    try:\n",
        "        filename = take_photo()\n",
        "        print('Saved to {}'.format(filename))\n",
        "        \n",
        "        # Show the image which was just taken.\n",
        "        display(Image(filename))\n",
        "    except Exception as err:\n",
        "    # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "    # grant the page permission to access it.\n",
        "        print(str(err))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}